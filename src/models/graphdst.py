"""
Multi-Layer GraphDST Model

This module implements the core GraphDST architecture with:
- Schema-aware Graph Convolution layers
- Cross-domain Graph Attention
- Temporal Graph Recurrence  
- Multi-task prediction heads
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import softmax
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass


@dataclass
class GraphDSTConfig:
    """Configuration for GraphDST model"""
    # Model dimensions
    hidden_dim: int = 768
    num_gnn_layers: int = 3
    num_attention_heads: int = 8
    dropout: float = 0.1
    
    # Text encoder
    text_encoder: str = "bert-base-uncased"
    max_sequence_length: int = 512
    
    # Graph structure
    num_domains: int = 5
    num_slots: int = 37
    max_turn_length: int = 20
    
    # Training
    learning_rate: float = 2e-5
    weight_decay: float = 0.01
    warmup_steps: int = 1000
    
    # Slot categorization threshold
    categorical_threshold: int = 50


class MultiHeadGraphAttention(nn.Module):
    """Multi-head attention for graph neural networks"""
    
    def __init__(self, input_dim: int, output_dim: int, num_heads: int = 8, dropout: float = 0.1):
        """
        Initialize multi-head graph attention
        
        Args:
            input_dim: Input feature dimension
            output_dim: Output feature dimension  
            num_heads: Number of attention heads
            dropout: Dropout rate
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_heads = num_heads
        self.head_dim = output_dim // num_heads
        self.dropout = dropout
        
        assert output_dim % num_heads == 0, "output_dim must be divisible by num_heads"
        
        # Query, Key, Value projections for all heads
        self.w_q = nn.Linear(input_dim, output_dim)
        self.w_k = nn.Linear(input_dim, output_dim)
        self.w_v = nn.Linear(input_dim, output_dim)
        self.w_o = nn.Linear(output_dim, output_dim)
        
        # Attention edge projection (optional edge features)
        self.edge_proj = nn.Linear(input_dim, num_heads)
        
        self.dropout_layer = nn.Dropout(dropout)
        self.scale = math.sqrt(self.head_dim)
        
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, 
                edge_index: torch.Tensor, edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass of graph attention
        
        Args:
            query: Query features (num_nodes, input_dim)
            key: Key features (num_nodes, input_dim)
            value: Value features (num_nodes, input_dim)
            edge_index: Edge indices (2, num_edges)
            edge_attr: Edge attributes (num_edges, edge_dim)
        
        Returns:
            Updated node features (num_nodes, output_dim)
        """
        num_nodes = query.size(0)
        num_edges = edge_index.size(1)
        
        # Project to Q, K, V
        Q = self.w_q(query).view(num_nodes, self.num_heads, self.head_dim)  # (N, H, D)
        K = self.w_k(key).view(num_nodes, self.num_heads, self.head_dim)    # (N, H, D)
        V = self.w_v(value).view(num_nodes, self.num_heads, self.head_dim)  # (N, H, D)
        
        # Compute attention scores along edges
        src, dst = edge_index[0], edge_index[1]
        
        # Get Q and K for each edge
        Q_dst = Q[dst]  # (E, H, D) - queries at destination nodes
        K_src = K[src]  # (E, H, D) - keys at source nodes
        
        # Compute attention scores
        attn_scores = (Q_dst * K_src).sum(dim=-1) / self.scale  # (E, H)
        
        # Add edge attributes if provided
        if edge_attr is not None:
            edge_weights = self.edge_proj(edge_attr)  # (E, H)
            attn_scores = attn_scores + edge_weights
        
        # Apply softmax per destination node
        attn_weights = softmax(attn_scores, dst, num_nodes=num_nodes)  # (E, H)
        attn_weights = self.dropout_layer(attn_weights)
        
        # Aggregate values
        V_src = V[src]  # (E, H, D)
        attn_weights_expanded = attn_weights.unsqueeze(-1)  # (E, H, 1)
        
        # Weighted sum of values
        messages = V_src * attn_weights_expanded  # (E, H, D)
        
        # Aggregate messages to destination nodes
        output = torch.zeros(num_nodes, self.num_heads, self.head_dim, 
                           device=query.device, dtype=query.dtype)
        output.index_add_(0, dst, messages)
        
        # Reshape and apply output projection
        output = output.view(num_nodes, self.output_dim)
        output = self.w_o(output)
        output = self.dropout_layer(output)
        
        return output


class SchemaGCNLayer(nn.Module):
    """Schema-aware Graph Convolution Layer using PyTorch Geometric"""
    
    def __init__(self, input_dim: int, output_dim: int, dropout: float = 0.1):
        """
        Initialize Schema GCN layer
        
        Args:
            input_dim: Input feature dimension
            output_dim: Output feature dimension
            dropout: Dropout rate
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.dropout = dropout
        
        # Convolution layers for different node types
        self.domain_conv = HeteroGraphConv(input_dim, output_dim)
        self.slot_conv = HeteroGraphConv(input_dim, output_dim)
        self.value_conv = HeteroGraphConv(input_dim, output_dim)
        
        # Layer normalization for each node type
        self.domain_norm = nn.LayerNorm(output_dim)
        self.slot_norm = nn.LayerNorm(output_dim)
        self.value_norm = nn.LayerNorm(output_dim)
        
        # Dropout
        self.dropout_layer = nn.Dropout(dropout)
        
        # Activation
        self.activation = nn.ReLU()
        
    def forward(self, x_dict: Dict[str, torch.Tensor], 
                edge_index_dict: Dict[Tuple[str, str, str], torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Forward pass of schema GCN
        
        Args:
            x_dict: Node features dict {'domain': tensor, 'slot': tensor, 'value': tensor}
            edge_index_dict: Edge indices dict with keys like ('domain', 'connected', 'domain')
        
        Returns:
            Updated node features dictionary
        """
        # Process domain nodes
        domain_out = self._process_domain_nodes(x_dict, edge_index_dict)
        
        # Process slot nodes (receives info from domains)
        slot_out = self._process_slot_nodes(x_dict, edge_index_dict)
        
        # Process value nodes (receives info from slots)
        value_out = self._process_value_nodes(x_dict, edge_index_dict)
        
        return {
            'domain': domain_out,
            'slot': slot_out,
            'value': value_out
        }
    
    def _process_domain_nodes(self, x_dict: Dict[str, torch.Tensor], 
                             edge_index_dict: Dict) -> torch.Tensor:
        """Process domain nodes with intra-domain connections"""
        domain_features = x_dict['domain']
        
        # Self-loops for domains
        output = self.domain_conv(domain_features, domain_features, 
                                 edge_index_dict.get(('domain', 'connected', 'domain')))
        
        # Apply normalization and residual connection
        output = self.domain_norm(output + domain_features)
        output = self.activation(output)
        output = self.dropout_layer(output)
        
        return output
    
    def _process_slot_nodes(self, x_dict: Dict[str, torch.Tensor],
                           edge_index_dict: Dict) -> torch.Tensor:
        """Process slot nodes with domain-slot connections"""
        slot_features = x_dict['slot']
        domain_features = x_dict['domain']
        
        # Aggregate information from domains
        edge_index = edge_index_dict.get(('domain', 'contains', 'slot'))
        
        if edge_index is not None:
            output = self.slot_conv(slot_features, domain_features, edge_index)
        else:
            output = slot_features
        
        # Apply normalization and residual connection
        output = self.slot_norm(output + slot_features)
        output = self.activation(output)
        output = self.dropout_layer(output)
        
        return output
    
    def _process_value_nodes(self, x_dict: Dict[str, torch.Tensor],
                            edge_index_dict: Dict) -> torch.Tensor:
        """Process value nodes with slot-value connections"""
        value_features = x_dict['value']
        slot_features = x_dict['slot']
        
        # Aggregate information from slots
        edge_index = edge_index_dict.get(('slot', 'accepts', 'value'))
        
        if edge_index is not None:
            output = self.value_conv(value_features, slot_features, edge_index)
        else:
            output = value_features
        
        # Apply normalization and residual connection
        output = self.value_norm(output + value_features)
        output = self.activation(output)
        output = self.dropout_layer(output)
        
        return output


class HeteroGraphConv(nn.Module):
    """Simple heterogeneous graph convolution for message passing"""
    
    def __init__(self, input_dim: int, output_dim: int):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # Linear transformation for target nodes
        self.lin_target = nn.Linear(input_dim, output_dim)
        # Linear transformation for source nodes
        self.lin_source = nn.Linear(input_dim, output_dim)
        
    def forward(self, x_target: torch.Tensor, x_source: torch.Tensor,
                edge_index: Optional[torch.Tensor]) -> torch.Tensor:
        """
        Message passing from source to target nodes
        
        Args:
            x_target: Target node features (num_target, input_dim)
            x_source: Source node features (num_source, input_dim)
            edge_index: Edge indices (2, num_edges) with [source_idx, target_idx]
        
        Returns:
            Updated target node features (num_target, output_dim)
        """
        if edge_index is None or edge_index.size(1) == 0:
            # No edges, just transform target features
            return self.lin_target(x_target)
        
        # Transform features
        x_target_transformed = self.lin_target(x_target)
        x_source_transformed = self.lin_source(x_source)
        
        # Message passing
        src_idx, dst_idx = edge_index[0], edge_index[1]
        
        # Aggregate messages from source to target
        messages = x_source_transformed[src_idx]  # (num_edges, output_dim)
        
        # Sum aggregation
        output = torch.zeros_like(x_target_transformed)
        output.index_add_(0, dst_idx, messages)
        
        # Normalize by degree (number of incoming edges)
        degree = torch.zeros(x_target.size(0), device=x_target.device)
        degree.index_add_(0, dst_idx, torch.ones(dst_idx.size(0), device=x_target.device))
        degree = degree.clamp(min=1).unsqueeze(-1)
        
        output = output / degree + x_target_transformed
        
        return output


class CrossDomainGATLayer(nn.Module):
    """Cross-domain Graph Attention Layer"""
    
    def __init__(self, input_dim: int, output_dim: int, num_heads: int = 8, dropout: float = 0.1):
        """
        Initialize cross-domain GAT layer
        
        Args:
            input_dim: Input feature dimension
            output_dim: Output feature dimension
            num_heads: Number of attention heads
            dropout: Dropout rate
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_heads = num_heads
        self.dropout = dropout
        
        # Multi-head attention for different edge types
        self.slot_attention = MultiHeadGraphAttention(input_dim, output_dim, num_heads, dropout)
        self.domain_attention = MultiHeadGraphAttention(input_dim, output_dim, num_heads, dropout)
        
        # Residual connections
        self.slot_residual = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()
        self.domain_residual = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()
        
    def forward(self, x_dict: Dict[str, torch.Tensor], 
                edge_index_dict: Dict[Tuple[str, str, str], torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Forward pass of cross-domain GAT
        
        Args:
            x_dict: Node features dictionary
            edge_index_dict: Edge indices dictionary
        
        Returns:
            Updated node features with cross-domain information
        """
        output_dict = {}
        
        # Cross-domain slot attention (slots attending to other slots)
        slot_features = x_dict['slot']
        slot_edges = edge_index_dict.get(('slot', 'similar', 'slot'))
        
        if slot_edges is not None and slot_edges.size(1) > 0:
            slot_updated = self.slot_attention(
                query=slot_features,
                key=slot_features, 
                value=slot_features,
                edge_index=slot_edges
            )
            # Add residual connection
            slot_updated = slot_updated + self.slot_residual(slot_features)
            output_dict['slot'] = slot_updated
        else:
            output_dict['slot'] = slot_features
        
        # Cross-domain attention for domains
        domain_features = x_dict['domain']
        domain_edges = edge_index_dict.get(('domain', 'connected', 'domain'))
        
        if domain_edges is not None and domain_edges.size(1) > 0:
            domain_updated = self.domain_attention(
                query=domain_features,
                key=domain_features,
                value=domain_features, 
                edge_index=domain_edges
            )
            # Add residual connection
            domain_updated = domain_updated + self.domain_residual(domain_features)
            output_dict['domain'] = domain_updated
        else:
            output_dict['domain'] = domain_features
        
        # Value nodes remain unchanged (or copy from input)
        if 'value' in x_dict:
            output_dict['value'] = x_dict['value']
        
        return output_dict


class TemporalGRULayer(nn.Module):
    """Temporal GRU layer for dialog context modeling"""
    
    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int = 2, dropout: float = 0.1):
        """
        Initialize temporal GRU layer
        
        Args:
            input_dim: Input feature dimension
            hidden_dim: Hidden state dimension
            num_layers: Number of GRU layers
            dropout: Dropout rate
        """
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout
        
        # GRU for temporal modeling
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=False
        )
        
        # Positional embeddings for turn positions
        self.max_turns = 20  # Maximum dialog length
        self.turn_embedding = nn.Embedding(self.max_turns, input_dim)
        
        # Layer norm
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Attention over turns
        self.turn_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
    def forward(self, dialog_sequence: torch.Tensor, 
                turn_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass of temporal GRU
        
        Args:
            dialog_sequence: Sequence of turn representations (batch, max_turns, hidden_dim)
            turn_mask: Mask for valid turns (batch, max_turns)
        
        Returns:
            Tuple of (contextualized turn representations, final hidden state)
        """
        batch_size, max_turns, hidden_dim = dialog_sequence.shape
        
        # Add positional embeddings
        positions = torch.arange(max_turns, device=dialog_sequence.device)
        position_embeds = self.turn_embedding(positions).unsqueeze(0)  # (1, max_turns, input_dim)
        dialog_sequence = dialog_sequence + position_embeds
        
        # Apply GRU for temporal modeling
        gru_output, hidden_state = self.gru(dialog_sequence)  # (batch, max_turns, hidden_dim)
        
        # Apply layer normalization
        gru_output = self.layer_norm(gru_output)
        
        # Apply self-attention over turns
        if turn_mask is not None:
            # Convert mask to attention mask format (True = masked position)
            attn_mask = ~turn_mask  # Invert: False = valid, True = masked
        else:
            attn_mask = None
        
        contextualized, _ = self.turn_attention(
            query=gru_output,
            key=gru_output,
            value=gru_output,
            key_padding_mask=attn_mask
        )
        
        # Residual connection
        output = gru_output + contextualized
        
        return output, hidden_state


class MultiTaskHeads(nn.Module):
    """Multi-task prediction heads for DST"""
    
    def __init__(self, config: GraphDSTConfig, slot_info: Dict):
        """
        Initialize multi-task heads
        
        Args:
            config: Model configuration
            slot_info: Information about slots (categorical vs span)
        """
        super().__init__()
        self.config = config
        self.slot_info = slot_info
        
        hidden_dim = config.hidden_dim
        
        # Domain classification head (multi-label binary classification)
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(hidden_dim // 2, config.num_domains)
        )
        
        # Slot activation heads (binary classification for each slot)
        self.slot_classifiers = nn.ModuleDict()
        for slot_name in slot_info.get('slot_names', []):
            self.slot_classifiers[slot_name.replace('-', '_')] = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),  # *2 for concatenated features
                nn.ReLU(),
                nn.Dropout(config.dropout),
                nn.Linear(hidden_dim, 2)  # Binary: active or not
            )
        
        # Value prediction heads for categorical slots
        self.categorical_heads = nn.ModuleDict()
        for slot_name, vocab_size in slot_info.get('categorical_slots', {}).items():
            self.categorical_heads[slot_name.replace('-', '_')] = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(config.dropout),
                nn.Linear(hidden_dim, vocab_size)
            )
        
        # Span prediction heads for extractive slots
        self.span_start_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(hidden_dim, 1)
        )
        
        self.span_end_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, schema_features: Dict[str, torch.Tensor], 
                dialog_features: torch.Tensor,
                utterance_features: torch.Tensor,
                slot_indices: Optional[Dict[str, int]] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass of multi-task heads
        
        Args:
            schema_features: Features from schema graph {'domain': (N_d, D), 'slot': (N_s, D)}
            dialog_features: Features from dialog context (batch, seq_len, hidden_dim)
            utterance_features: Current utterance CLS features (batch, hidden_dim)
            slot_indices: Mapping from slot names to indices in schema graph
        
        Returns:
            Dictionary of predictions for all tasks
        """
        batch_size = utterance_features.size(0)
        predictions = {}
        
        # 1. Domain prediction
        # Use CLS token + mean of domain node features
        domain_node_features = schema_features.get('domain')  # (num_domains, hidden_dim)
        if domain_node_features is not None:
            # Average domain features and expand to batch
            avg_domain_features = domain_node_features.mean(dim=0, keepdim=True)  # (1, hidden_dim)
            domain_input = utterance_features + avg_domain_features
        else:
            domain_input = utterance_features
        
        predictions['domains'] = self.domain_classifier(domain_input)  # (batch, num_domains)
        
        # 2. Slot activation prediction
        predictions['slot_activations'] = {}
        slot_node_features = schema_features.get('slot')  # (num_slots, hidden_dim)
        
        for slot_name in self.slot_info.get('slot_names', []):
            slot_key = slot_name.replace('-', '_')
            
            # Get specific slot features from graph
            if slot_indices and slot_name in slot_indices and slot_node_features is not None:
                slot_idx = slot_indices[slot_name]
                slot_feature = slot_node_features[slot_idx:slot_idx+1]  # (1, hidden_dim)
                slot_feature = slot_feature.expand(batch_size, -1)  # (batch, hidden_dim)
            else:
                # Fallback: use mean of all slot features
                slot_feature = torch.zeros_like(utterance_features)
            
            # Concatenate utterance and slot features
            slot_input = torch.cat([utterance_features, slot_feature], dim=-1)
            
            slot_logits = self.slot_classifiers[slot_key](slot_input)  # (batch, 2)
            predictions['slot_activations'][slot_name] = slot_logits
        
        # 3. Value prediction
        predictions['values'] = {}
        
        # Categorical value prediction
        for slot_name in self.slot_info.get('categorical_slot_names', []):
            slot_key = slot_name.replace('-', '_')
            
            # Get slot features
            if slot_indices and slot_name in slot_indices and slot_node_features is not None:
                slot_idx = slot_indices[slot_name]
                slot_feature = slot_node_features[slot_idx:slot_idx+1]
                slot_feature = slot_feature.expand(batch_size, -1)
            else:
                slot_feature = torch.zeros_like(utterance_features)
            
            value_input = torch.cat([utterance_features, slot_feature], dim=-1)
            value_logits = self.categorical_heads[slot_key](value_input)
            predictions['values'][slot_name] = value_logits
        
        # Span value prediction (for all span-based slots)
        # Use token-level features from dialog_features
        if dialog_features.dim() == 3:  # (batch, seq_len, hidden_dim)
            seq_len = dialog_features.size(1)
            
            # Expand utterance features to match sequence length
            utterance_expanded = utterance_features.unsqueeze(1).expand(-1, seq_len, -1)
            span_input = torch.cat([dialog_features, utterance_expanded], dim=-1)
            
            # Predict start and end positions
            start_logits = self.span_start_head(span_input).squeeze(-1)  # (batch, seq_len)
            end_logits = self.span_end_head(span_input).squeeze(-1)  # (batch, seq_len)
            
            predictions['span_start'] = start_logits
            predictions['span_end'] = end_logits
        
        return predictions


class GraphDSTModel(nn.Module):
    """Main GraphDST Model combining all components"""
    
    def __init__(self, config: GraphDSTConfig, schema_builder, slot_info: Dict):
        """
        Initialize GraphDST model
        
        Args:
            config: Model configuration
            schema_builder: Schema graph builder
            slot_info: Slot information dictionary
        """
        super().__init__()
        self.config = config
        self.schema_builder = schema_builder
        self.slot_info = slot_info
        
        # Text encoder (BERT)
        from transformers import AutoModel
        self.text_encoder = AutoModel.from_pretrained(config.text_encoder)
        
        # Feature projection to match hidden_dim
        encoder_dim = self.text_encoder.config.hidden_size
        if encoder_dim != config.hidden_dim:
            self.feature_proj = nn.Linear(encoder_dim, config.hidden_dim)
        else:
            self.feature_proj = nn.Identity()
        
        # Graph neural network layers
        self.schema_gnn_layers = nn.ModuleList([
            SchemaGCNLayer(config.hidden_dim, config.hidden_dim, config.dropout)
            for _ in range(config.num_gnn_layers)
        ])
        
        self.cross_domain_gat_layers = nn.ModuleList([
            CrossDomainGATLayer(config.hidden_dim, config.hidden_dim, 
                              config.num_attention_heads, config.dropout)
            for _ in range(config.num_gnn_layers)
        ])
        
        # Temporal modeling
        self.temporal_gru = TemporalGRULayer(
            config.hidden_dim, config.hidden_dim, 
            num_layers=2, dropout=config.dropout
        )
        
        # Multi-task prediction heads
        self.prediction_heads = MultiTaskHeads(config, slot_info)
        
        # Schema graph (static, will be set during forward or initialization)
        self.schema_graph = None
        self.slot_indices = None
        
    def set_schema_graph(self, schema_graph: Dict, slot_indices: Dict[str, int]):
        """Set static schema graph"""
        self.schema_graph = schema_graph
        self.slot_indices = slot_indices
        
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,
                turn_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass of GraphDST model
        
        Args:
            input_ids: Tokenized input (batch, seq_len)
            attention_mask: Attention mask (batch, seq_len)
            turn_mask: Mask for valid turns (batch, max_turns) - optional for single-turn
        
        Returns:
            Dictionary of predictions for all tasks
        """
        batch_size = input_ids.size(0)
        
        # 1. Encode text with BERT
        encoder_output = self.text_encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # Get hidden states and CLS token
        hidden_states = encoder_output.last_hidden_state  # (batch, seq_len, encoder_dim)
        cls_token = hidden_states[:, 0]  # (batch, encoder_dim)
        
        # Project to model hidden dimension
        hidden_states = self.feature_proj(hidden_states)  # (batch, seq_len, hidden_dim)
        cls_token = self.feature_proj(cls_token)  # (batch, hidden_dim)
        
        # 2. Process schema graph
        schema_features = self._process_schema_graph(cls_token)
        
        # 3. Process dialog context (for multi-turn scenarios)
        # For now, treat each example independently
        dialog_features = hidden_states
        
        # 4. Multi-task prediction
        predictions = self.prediction_heads(
            schema_features=schema_features,
            dialog_features=dialog_features,
            utterance_features=cls_token,
            slot_indices=self.slot_indices
        )
        
        return predictions
    
    def _process_schema_graph(self, text_features: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process schema graph with GNN layers
        
        Args:
            text_features: Text features to condition on (batch, hidden_dim)
        
        Returns:
            Updated schema node features
        """
        if self.schema_graph is None:
            # Return empty dict if no schema graph is set
            return {}
        
        # Get initial node features (these should be learned embeddings)
        x_dict = {
            'domain': self.schema_graph['domain']['x'],
            'slot': self.schema_graph['slot']['x'],
            'value': self.schema_graph['value']['x']
        }
        
        edge_index_dict = self.schema_graph['edge_index_dict']
        
        # Apply GNN layers
        for schema_layer, gat_layer in zip(self.schema_gnn_layers, self.cross_domain_gat_layers):
            # Schema-aware convolution
            x_dict = schema_layer(x_dict, edge_index_dict)
            
            # Cross-domain attention
            x_dict = gat_layer(x_dict, edge_index_dict)
        
        return x_dict
    
    def compute_loss(self, predictions: Dict[str, torch.Tensor], 
                    labels: Dict[str, torch.Tensor],
                    loss_weights: Optional[Dict[str, float]] = None) -> Dict[str, torch.Tensor]:
        """
        Compute multi-task loss
        
        Args:
            predictions: Model predictions
            labels: Ground truth labels
            loss_weights: Optional weights for each loss component
        
        Returns:
            Dictionary with total loss and component losses
        """
        if loss_weights is None:
            loss_weights = {'domain': 1.0, 'slot': 1.0, 'value': 1.0}
        
        losses = {}
        device = next(self.parameters()).device
        
        # 1. Domain classification loss (multi-label BCE)
        if 'domain_labels' in labels and 'domains' in predictions:
            domain_loss = F.binary_cross_entropy_with_logits(
                predictions['domains'],
                labels['domain_labels'].float()
            )
            losses['domain'] = domain_loss * loss_weights.get('domain', 1.0)
        
        # 2. Slot activation losses (binary classification for each slot)
        slot_losses = []
        for slot_name in self.slot_info.get('slot_names', []):
            label_key = f'{slot_name}_active'
            
            if label_key in labels and slot_name in predictions['slot_activations']:
                slot_loss = F.cross_entropy(
                    predictions['slot_activations'][slot_name],
                    labels[label_key].long()
                )
                slot_losses.append(slot_loss)
        
        if slot_losses:
            losses['slot'] = torch.stack(slot_losses).mean() * loss_weights.get('slot', 1.0)
        
        # 3. Value prediction losses
        value_losses = []
        
        # Categorical value losses
        for slot_name in self.slot_info.get('categorical_slot_names', []):
            label_key = f'{slot_name}_value'
            
            if label_key in labels and slot_name in predictions['values']:
                # Only compute loss for active slots
                active_key = f'{slot_name}_active'
                if active_key in labels:
                    active_mask = labels[active_key].bool()
                    if active_mask.any():
                        value_loss = F.cross_entropy(
                            predictions['values'][slot_name][active_mask],
                            labels[label_key][active_mask].long(),
                            reduction='mean'
                        )
                        value_losses.append(value_loss)
        
        # Span value losses
        if 'span_start' in predictions and 'span_start_labels' in labels:
            start_loss = F.cross_entropy(
                predictions['span_start'],
                labels['span_start_labels'].long(),
                ignore_index=-1  # Ignore padding positions
            )
            value_losses.append(start_loss)
        
        if 'span_end' in predictions and 'span_end_labels' in labels:
            end_loss = F.cross_entropy(
                predictions['span_end'],
                labels['span_end_labels'].long(),
                ignore_index=-1
            )
            value_losses.append(end_loss)
        
        if value_losses:
            losses['value'] = torch.stack(value_losses).mean() * loss_weights.get('value', 1.0)
        
        # Total loss
        total_loss = sum(losses.values())
        losses['total'] = total_loss
        
        return losses


# Factory function to create model
def create_graphdst_model(ontology_path: str, config: GraphDSTConfig = None) -> GraphDSTModel:
    """
    Create GraphDST model with default configuration
    
    Args:
        ontology_path: Path to ontology.json file
        config: Optional model configuration
    
    Returns:
        Initialized GraphDST model
    """
    if config is None:
        config = GraphDSTConfig()
    
    # Import schema builder
    from src.data.schema_graph import SchemaGraphBuilder
    
    # Create schema builder
    schema_builder = SchemaGraphBuilder(ontology_path)
    
    # Get slot information
    slot_info = {
        'slot_names': schema_builder.slots,
        'categorical_slot_names': schema_builder.categorical_slots,
        'span_slot_names': schema_builder.span_slots,
        'categorical_slots': {}  # vocab_size for each categorical slot
    }
    
    # Build vocab sizes for categorical slots
    for slot in schema_builder.categorical_slots:
        if slot in schema_builder.ontology:
            slot_info['categorical_slots'][slot] = len(schema_builder.ontology[slot])
    
    # Create model
    model = GraphDSTModel(config, schema_builder, slot_info)
    
    # Build and set schema graph
    try:
        schema_graph_data = schema_builder.build_hetero_graph_dict()
        slot_indices = {slot: i for i, slot in enumerate(schema_builder.slots)}
        model.set_schema_graph(schema_graph_data, slot_indices)
    except Exception as e:
        print(f"Warning: Could not build schema graph: {e}")
        print("Model will work without pre-built schema graph")
    
    return model


if __name__ == "__main__":
    print("=" * 60)
    print("GraphDST Model with PyTorch Implementation")
    print("=" * 60)
    
    # Example model creation
    config = GraphDSTConfig(
        hidden_dim=768,
        num_gnn_layers=3,
        num_attention_heads=8,
        dropout=0.1,
        num_domains=5,
        num_slots=37
    )
    
    print(f"\nModel Configuration:")
    print(f"  - Hidden dimension: {config.hidden_dim}")
    print(f"  - GNN layers: {config.num_gnn_layers}")
    print(f"  - Attention heads: {config.num_attention_heads}")
    print(f"  - Dropout: {config.dropout}")
    print(f"  - Number of domains: {config.num_domains}")
    print(f"  - Number of slots: {config.num_slots}")
    
    # Try to create model (will fail if ontology not found, but shows structure)
    ontology_path = "../../data/ontology.json"
    
    try:
        model = create_graphdst_model(ontology_path, config)
        
        print(f"\nâœ“ Model created successfully!")
        print(f"\nModel Architecture:")
        print(f"  - Text Encoder: {config.text_encoder}")
        print(f"  - Schema GCN Layers: {len(model.schema_gnn_layers)}")
        print(f"  - Cross-Domain GAT Layers: {len(model.cross_domain_gat_layers)}")
        print(f"  - Temporal GRU: {model.temporal_gru.__class__.__name__}")
        print(f"  - Prediction Heads: {model.prediction_heads.__class__.__name__}")
        
        # Count parameters
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"\nModel Statistics:")
        print(f"  - Total parameters: {total_params:,}")
        print(f"  - Trainable parameters: {trainable_params:,}")
        print(f"  - Model size: ~{total_params * 4 / (1024**2):.2f} MB")
        
        print(f"\nSlot Information:")
        print(f"  - Total slots: {len(model.slot_info['slot_names'])}")
        print(f"  - Categorical slots: {len(model.slot_info['categorical_slot_names'])}")
        print(f"  - Span slots: {len(model.slot_info['span_slot_names'])}")
        
    except FileNotFoundError:
        print(f"\nâš  Ontology file not found at: {ontology_path}")
        print(f"  Model structure defined but not instantiated")
    except Exception as e:
        print(f"\nâš  Error creating model: {e}")
        print(f"  Model structure is still valid for import and use")
    
    print("\n" + "=" * 60)
    print("PyTorch GNN Implementation Complete!")
    print("=" * 60)
    print("\nKey Features Implemented:")
    print("  âœ“ Multi-head Graph Attention with edge features")
    print("  âœ“ Schema-aware Graph Convolution (heterogeneous)")
    print("  âœ“ Cross-domain Graph Attention Layer")
    print("  âœ“ Temporal GRU with positional embeddings")
    print("  âœ“ Multi-task prediction heads (domain/slot/value)")
    print("  âœ“ Comprehensive loss computation")
    print("  âœ“ Full PyTorch nn.Module implementation")
    print("\nReady for training! ðŸš€")
    print("=" * 60)